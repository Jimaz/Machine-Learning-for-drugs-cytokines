{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Machine Learning models\n",
    "\n",
    "This is the first script to use a set of ML method with default parameters in order to obtain baseline results. We will use:\n",
    "1. Nearest Neighbors\n",
    "2. Linear SVM\n",
    "3. RBF SVM\n",
    "4. Gaussian Process\n",
    "5. AdaBoost\n",
    "6. Naive Bayes\n",
    "7. Quadratic Discriminant Analysis\n",
    "8. Neural Net\n",
    "9. Decision Tree\n",
    "10. Random Forest\n",
    "11. XGBoost\n",
    "\n",
    "*Note: More advanced hyperparameter search will be done in future scripts!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the folder with datasets and the list with dataset files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset folder\n",
    "WorkingFolder  = './datasets/'\n",
    "BasicMLResults = 'ML_basic.csv' # a file with all the statistis for ML models\n",
    "\n",
    "# define the list of datasets (manually or script)\n",
    "listFiles_tr = ['test_tr.csv','test2_tr.csv'] # manually set of training files\n",
    "#listFiles_tr = [col for col in os.listdir(WorkingFolder) if ('_MA_tr.' in col)]\n",
    "\n",
    "listFiles_ts = ['test_ts.csv','test2_ts.csv'] # manually set of test files\n",
    "#listFiles_ts = [col for col in os.listdir(WorkingFolder) if ('_MA_ts.' in col)]\n",
    "\n",
    "# Split details\n",
    "seed = 0          # for reproductibility\n",
    "\n",
    "# output variable\n",
    "outVar = 'Lij'    \n",
    "\n",
    "# parameter for ballanced (equal number of examples in all classes) and non-ballanced dataset \n",
    "class_weight = \"balanced\" # use None for ballanced datasets!\n",
    "\n",
    "# remember ballanced datasets!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we are running a base line set of models with default parameters. In the next step, a grid search will be used for better models. We already have the training and test datasets as files from the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Basic Machine Learning ...\n",
      "class 0 = 147 class 1 = 52\n",
      "Ratio 1/0 1 : 2.8269\n",
      "**************************************\n",
      "Dataset MLmethod ACC AUROC f1-score recall prec\n",
      "test_tr.csv Nearest Neighbors 0.83 0.67 0.50 [0.94736842 0.4       ] [0.85714286 0.66666667]\n",
      "test_tr.csv Linear SVM 0.79 0.50 0.00 [1. 0.] [0.79166667 0.        ]\n",
      "test_tr.csv RBF SVM 0.96 0.90 0.89 [1.  0.8] [0.95 1.  ]\n",
      "test_tr.csv Gaussian Process 0.79 0.50 0.00 [1. 0.] [0.79166667 0.        ]\n",
      "test_tr.csv AdaBoost 0.96 0.90 0.89 [1.  0.8] [0.95 1.  ]\n",
      "test_tr.csv Naive Bayes 0.21 0.50 0.34 [0. 1.] [0.         0.20833333]\n",
      "test_tr.csv QDA 0.38 0.61 0.40 [0.21052632 1.        ] [1.   0.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\discriminant_analysis.py:686: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tr.csv Neural Net 0.88 0.70 0.57 [1.  0.4] [0.86363636 1.        ]\n",
      "test_tr.csv Decision Tree 0.96 0.90 0.89 [1.  0.8] [0.95 1.  ]\n",
      "test_tr.csv Random Forest 0.96 0.90 0.89 [1.  0.8] [0.95 1.  ]\n",
      "test_tr.csv XGBoost 1.00 1.00 1.00 [1. 1.] [1. 1.]\n",
      "class 0 = 147 class 1 = 52\n",
      "Ratio 1/0 1 : 2.8269\n",
      "**************************************\n",
      "Dataset MLmethod ACC AUROC f1-score recall prec\n",
      "test2_tr.csv Nearest Neighbors 0.79 0.73 0.60 [0.85714286 0.6       ] [0.85714286 0.6       ]\n",
      "test2_tr.csv Linear SVM 0.74 0.50 0.00 [1. 0.] [0.73684211 0.        ]\n",
      "test2_tr.csv RBF SVM 0.95 0.90 0.89 [1.  0.8] [0.93333333 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\discriminant_analysis.py:686: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2_tr.csv Gaussian Process 0.74 0.50 0.00 [1. 0.] [0.73684211 0.        ]\n",
      "test2_tr.csv AdaBoost 0.84 0.76 0.67 [0.92857143 0.6       ] [0.86666667 0.75      ]\n",
      "test2_tr.csv Naive Bayes 0.68 0.66 0.50 [0.71428571 0.6       ] [0.83333333 0.42857143]\n",
      "test2_tr.csv QDA 0.26 0.50 0.42 [0. 1.] [0.         0.26315789]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2_tr.csv Neural Net 0.84 0.70 0.57 [1.  0.4] [0.82352941 1.        ]\n",
      "test2_tr.csv Decision Tree 0.95 0.90 0.89 [1.  0.8] [0.93333333 1.        ]\n",
      "test2_tr.csv Random Forest 1.00 1.00 1.00 [1. 1.] [1. 1.]\n",
      "test2_tr.csv XGBoost 0.89 0.86 0.80 [0.92857143 0.8       ] [0.92857143 0.8       ]\n",
      "---> Saving results ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('-> Basic Machine Learning ...')\n",
    "basicResults = [] #list with results\n",
    "\n",
    "MLnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "          \"AdaBoost\", \"Naive Bayes\", \"QDA\", \"Neural Net\",\n",
    "           \"Decision Tree\", \"Random Forest\", \"XGBoost\"]\n",
    "\n",
    "# class_weight={0: w0, 1: w1}\n",
    "#for each dataset\n",
    "for f in range(len(listFiles_tr)):\n",
    "    newFile_tr = listFiles_tr[f]\n",
    "    newFile_ts = listFiles_ts[f]\n",
    "    \n",
    "    # read training set as dataframe\n",
    "    # print('---> Reading data:', newFile_tr, '...')\n",
    "    df_tr = pd.read_csv(os.path.join(WorkingFolder, newFile_tr))\n",
    "    X_tr = df_tr.drop(outVar, axis = 1) # remove output variable from input features\n",
    "    y_tr = df_tr[outVar]                # get only the output variable\n",
    "        \n",
    "    # read test set as dataframe\n",
    "    # print('---> Reading data:', newFile_ts, '...')\n",
    "    df_ts = pd.read_csv(os.path.join(WorkingFolder, newFile_ts))\n",
    "    X_ts = df_ts.drop(outVar, axis = 1) # remove output variable from input features\n",
    "    y_ts = df_ts[outVar]                # get only the output variable\n",
    "            \n",
    "    # get only array data for train\n",
    "    X_tr_data = X_tr.values # get values of features\n",
    "    y_tr_data = y_tr.values # get output values\n",
    "    # get only array data for test\n",
    "    X_ts_data = X_ts.values # get values of features\n",
    "    y_ts_data = y_ts.values # get output values\n",
    "        \n",
    "    # default weights are egual for positive and negative classes = 1 (1:1)\n",
    "    # we suppose we have a ballanced dataset\n",
    "    # NOTE: sample_weights from fit() is about the importance of each point! not the class ballance!\n",
    "    w0 = w1 = 1\n",
    "\n",
    "    # check ratio for the classes (no of examples for each class)\n",
    "    # Class counts\n",
    "    target_count = y_tr.value_counts()\n",
    "    nClass0 = target_count[0]\n",
    "    nClass1 = target_count[1]\n",
    "    print('class 0 =', nClass0, 'class 1 =', nClass1)\n",
    "\n",
    "    # calculate the weight ratio\n",
    "    if nClass0 > nClass1: # if class 0 has more examples\n",
    "        w0 = round(nClass0/nClass1, 4) #round with 4 decimals only !\n",
    "        w1 = 1\n",
    "    else:\n",
    "        if nClass0 < nClass1: # if class 1 has more examples\n",
    "            w0 = 1\n",
    "            w1 = round(nClass1/nClass0, 4)\n",
    "\n",
    "    print('Ratio 1/0', w1, ':', w0)\n",
    "    \n",
    "    print('**************************************')\n",
    "    print('Dataset MLmethod ACC AUROC f1-score recall prec')\n",
    "    \n",
    "    # we are using scale_pos_weight for unballanced dataset!\n",
    "    MLclassifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(kernel=\"linear\", C=0.025, class_weight={0: w0, 1: w1}, random_state=seed),\n",
    "        SVC(gamma=2, C=1, class_weight={0: w0, 1: w1}, random_state=seed),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0), random_state = seed),\n",
    "        AdaBoostClassifier(random_state = seed),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        MLPClassifier(random_state = seed),\n",
    "        DecisionTreeClassifier(random_state = seed, class_weight={0: w0, 1: w1}),\n",
    "        RandomForestClassifier(random_state = seed, class_weight={0: w0, 1: w1}, n_jobs=-1),\n",
    "        XGBClassifier(nthread=-1, \n",
    "                      scale_pos_weight= int(w0/w1), # ratio weights negative / positive class\n",
    "                      seed=seed)]\n",
    "\n",
    "    \n",
    "    # for each ML method\n",
    "    for i in range(len(MLnames)):\n",
    "        iMLnames       = MLnames[i]\n",
    "        iMLclassifier  = MLclassifiers[i]\n",
    "        \n",
    "        #### ADD checking if files exists, do not calculate again ???\n",
    "        #### ADD time for each transformation and print it ???\n",
    "        \n",
    "        # TRY - EXCEPT for possible errors\n",
    "        \n",
    "        # training model\n",
    "        iMLclassifier.fit(X_tr_data, y_tr_data)\n",
    "        \n",
    "        # scores for test set\n",
    "        # score = iMLclassifier.score(X_ts_data, y_ts_data)\n",
    "        \n",
    "        # make predictions using test set\n",
    "        y_pred = iMLclassifier.predict(X_ts_data)\n",
    "\n",
    "        # calculate the statistics for the model (test ACC, AUROC, f1, recall, prec, confusion matrix)\n",
    "        ac    = accuracy_score(y_ts_data, y_pred)\n",
    "        auroc = roc_auc_score(y_ts_data, y_pred, average=\"weighted\", sample_weight=None)\n",
    "        f1    = f1_score(y_ts_data, y_pred, pos_label=1, average=\"binary\",\n",
    "                         sample_weight=None, labels=None)\n",
    "        recall= recall_score(y_ts_data, y_pred, average=None)\n",
    "        prec  = precision_score(y_ts_data, y_pred, average= None)\n",
    "        # cm = confusion_matrix(y_ts_data, y_pred)\n",
    "        \n",
    "        print(\"%s %s %4.2f %4.2f %4.2f %s %s\" % (newFile_tr, iMLnames, ac, auroc, f1, recall, prec))\n",
    "        basicResults.append((newFile_tr, iMLnames, ac, auroc, f1, recall, prec))\n",
    "\n",
    "# save the results\n",
    "# create a dataframe\n",
    "df_BasicMLResults = pd.DataFrame(basicResults, columns=['Dataset','MLmethod','ACC',\n",
    "                                                        'AUROC','f1-score','recall','prec'])\n",
    "print('---> Saving results ...')\n",
    "df_BasicMLResults.to_csv(BasicMLResults, index=False)\n",
    "print('! Please find your ML results in:', BasicMLResults)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these results, you could have an idea of the future ML model. Use your advanced scripts to find better parameters for the best models you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
