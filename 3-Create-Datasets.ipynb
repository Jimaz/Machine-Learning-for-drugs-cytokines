{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets\n",
    "## Scalling, Reduction and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset and/or the ballanced ones will be first splitted into separated files as training and test subsets using a **seed**. All the scalling and feature selection will be apply **only on training set**:\n",
    "- *Dataset split*: train, test sets; the train set will be divided into train and validation in future Machine Learning hyperparameter search for the best model with a ML method;\n",
    "- *Scalling* of train set using centering, standardization, etc.;\n",
    "- *Reduction* of train set dimension (after scalling): decrease the number of features using less dimensions/derived features;\n",
    "- *Feature selection* using train set (after scalling): decrease the number of features by keeping only the most important for the classification.\n",
    "\n",
    "Two CSV files will be create for each type of scalling, reduction or feature selection: *tr* - trainin and *ts* - test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split # for dataset split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the name of the original dataset, the folder and the prefix characters for each scalling, dimension reduction or feature selection. Each transformation will add a prefix to the previous name of the file.\n",
    "\n",
    "**You can used the original dataset that could be unballanced or the ballanced datasets obtained with previous scripts (one file only)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scalled datasets using normalized MA dataset\n",
    "# Two CSV files will be create for each type of scalling, reduction or feature selection\n",
    "WorkingFolder = './datasets/'\n",
    "\n",
    "# change this with ballanced datasets such as upsampl.ds_MA.csv or downsampl.ds_MA.csv\n",
    "# if you want to run all files, you should modify the entire script by looping all\n",
    "# transformation using a list of input files [original, undersampled, upsampled]\n",
    "sOrigDataSet  = 'ds_MA.csv'\n",
    "\n",
    "# Split details\n",
    "seed = 0          # for reproductibility\n",
    "\n",
    "test_size = 0.25  # train size = 1 - test_size\n",
    "outVar = 'Lij'    # output variable\n",
    "\n",
    "# Scalers: the files as prefix + original name\n",
    "# =================================================\n",
    "# Original (no scaling!), StandardScaler, MinMaxScaler, RobustScaler,\n",
    "# QuantileTransformer (normal), QuantileTransformer(uniform)\n",
    "\n",
    "# scaler prefix for file name\n",
    "scalerPrefix = ['o', 's', 'm', 'r', 'pyj', 'qn', 'qu']\n",
    "\n",
    "# sklearn scalers\n",
    "scalerList   = [None, StandardScaler(), MinMaxScaler(),\n",
    "                RobustScaler(quantile_range=(25, 75)),\n",
    "                PowerTransformer(method='yeo-johnson'),\n",
    "                QuantileTransformer(output_distribution='normal'),\n",
    "                QuantileTransformer(output_distribution='uniform')]\n",
    "\n",
    "# Dimension Reductions\n",
    "# ===================\n",
    "# PCA\n",
    "reductionPrefix = 'pca'\n",
    "\n",
    "# Feature selection\n",
    "# =================\n",
    "# RF feature selection, Univariate feature selection using chi-squared test,\n",
    "# Univariate feature selection with mutual information\n",
    "featSelPrefix = ['rffs','ufschi','ufsmi']\n",
    "\n",
    "# number of total features for reduction and selection\n",
    "noSelFeatures = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by reading the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Reading source dataset: ds_MA.csv ...\n",
      "Columns: 1519 Rows: 12766\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('-> Reading source dataset:',sOrigDataSet,'...')\n",
    "df = pd.read_csv(os.path.join(WorkingFolder, sOrigDataSet))\n",
    "print('Columns:',len(df.columns),'Rows:',len(df))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split\n",
    "\n",
    "First, split the dataset using stratification for non-ballanced datasets: the ratio between the classes is the same in training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Split of dataset in training and test ...\n",
      "X_train: (9574, 1518)\n",
      "y_train: (9574,)\n",
      "X_test: (3192, 1518)\n",
      "y_test: (3192,)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get features and ouput as dataframes\n",
    "print('--> Split of dataset in training and test ...')\n",
    "X = df.drop(outVar, axis = 1) # remove output variable from input features\n",
    "y = df[outVar]                # get only the output variable\n",
    "\n",
    "# get only the values for features and output (as arrays)\n",
    "Xdata = X.values # get values of features\n",
    "Ydata = y.values # get output values\n",
    "\n",
    "# split data in training and test sets (X = input features, y = output variable)\n",
    "# using a seed, test size (defined above) and stratification for un-ballanced classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xdata, Ydata,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=seed,\n",
    "                                                    stratify=Ydata)\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset scaling\n",
    "\n",
    "Two files will be saved for training and test sets for each scaling including non-scalling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Scaling dataset train and test:\n",
      "--> Original (no scaler!) ...\n",
      "---> Saving training: o.ds_MA_tr.csv  ...\n",
      "---> Saving test: o.ds_MA_ts.csv  ...\n",
      "--> Scaler: StandardScaler(copy=True, with_mean=True, with_std=True) ...\n",
      "---> Saving training: s.ds_MA_tr.csv  ...\n",
      "---> Saving test: s.ds_MA_ts.csv  ...\n",
      "--> Scaler: MinMaxScaler(copy=True, feature_range=(0, 1)) ...\n",
      "---> Saving training: m.ds_MA_tr.csv  ...\n",
      "---> Saving test: m.ds_MA_ts.csv  ...\n",
      "--> Scaler: RobustScaler(copy=True, quantile_range=(25, 75), with_centering=True,\n",
      "       with_scaling=True) ...\n",
      "---> Saving training: r.ds_MA_tr.csv  ...\n",
      "---> Saving test: r.ds_MA_ts.csv  ...\n",
      "--> Scaler: PowerTransformer(copy=True, method='yeo-johnson', standardize=True) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2784: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(est_var)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Saving training: pyj.ds_MA_tr.csv  ...\n",
      "---> Saving test: pyj.ds_MA_ts.csv  ...\n",
      "--> Scaler: QuantileTransformer(copy=True, ignore_implicit_zeros=False, n_quantiles=1000,\n",
      "          output_distribution='normal', random_state=None,\n",
      "          subsample=100000) ...\n",
      "---> Saving training: qn.ds_MA_tr.csv  ...\n",
      "---> Saving test: qn.ds_MA_ts.csv  ...\n",
      "--> Scaler: QuantileTransformer(copy=True, ignore_implicit_zeros=False, n_quantiles=1000,\n",
      "          output_distribution='uniform', random_state=None,\n",
      "          subsample=100000) ...\n",
      "---> Saving training: qu.ds_MA_tr.csv  ...\n",
      "---> Saving test: qu.ds_MA_ts.csv  ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Scale dataset\n",
    "print('-> Scaling dataset train and test:')\n",
    "\n",
    "for scaler in scalerList: # or scalerPrefix\n",
    "    \n",
    "    # new file name; we will add tr and ts + csv\n",
    "    newFile = scalerPrefix[scalerList.index(scaler)]+'.'+sOrigDataSet[:-4]\n",
    "    \n",
    "    # decide to scale or not\n",
    "    if scaler == None: # if it is the original dataset, do not scale!\n",
    "        print('--> Original (no scaler!) ...')\n",
    "        X_train_transf = X_train # do not modify train set\n",
    "        X_test_transf  = X_test  # do not modify test set\n",
    "        \n",
    "    else:              # if it is not the original dataset, apply scalers\n",
    "        print('--> Scaler:', str(scaler), '...')\n",
    "        X_train_transf = scaler.fit_transform(X_train) # use a scaler to modify only train set\n",
    "        X_test_transf  = scaler.transform(X_test)      # use the same transformation for test set\n",
    "\n",
    "    # Save the training scaled dataset\n",
    "    df_tr_scaler = pd.DataFrame(X_train_transf, columns=X.columns)\n",
    "    df_tr_scaler[outVar]= y_train\n",
    "    newFile_tr = newFile +'_tr.csv'\n",
    "\n",
    "    print('---> Saving training:', newFile_tr, ' ...')\n",
    "    df_tr_scaler.to_csv(os.path.join(WorkingFolder, newFile_tr), index=False)\n",
    "\n",
    "    # Save the test scaled dataset\n",
    "    df_ts_scaler = pd.DataFrame(X_test_transf, columns=X.columns)\n",
    "    df_ts_scaler[outVar]= y_test\n",
    "    newFile_ts = newFile +'_ts.csv'\n",
    "\n",
    "    print('---> Saving test:', newFile_ts, ' ...')\n",
    "    df_ts_scaler.to_csv(os.path.join(WorkingFolder, newFile_ts), index=False)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "PCA will be applied to all the previous scaled/non-scaled datasets because each scaler has a different sensibility to the outliers. The name of the transformed files could contain additional information such as the PCA explained variance. You could obtain different PCA transformed datasets using different variance values (ex: 0.99, 0.98, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> PCA reduction:\n",
      "* o.ds_MA\n",
      "PCA variance  : 0.9965242670292556\n",
      "PCA components: 2\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9965242670292556\n",
      "PCA components: 2\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9965242670292556\n",
      "PCA components: 2\n",
      "----->> Saving transformed dataset ...\n",
      "* s.ds_MA\n",
      "PCA variance  : 0.9901965172821318\n",
      "PCA components: 120\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9800331348542436\n",
      "PCA components: 96\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9708093876005063\n",
      "PCA components: 85\n",
      "----->> Saving transformed dataset ...\n",
      "* m.ds_MA\n",
      "PCA variance  : 0.9900896143915492\n",
      "PCA components: 79\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9800439871849247\n",
      "PCA components: 60\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9700000347373984\n",
      "PCA components: 50\n",
      "----->> Saving transformed dataset ...\n",
      "* r.ds_MA\n",
      "PCA variance  : 0.9963634456901177\n",
      "PCA components: 2\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9963634456901177\n",
      "PCA components: 2\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9963634456901177\n",
      "PCA components: 2\n",
      "----->> Saving transformed dataset ...\n",
      "* pyj.ds_MA\n",
      "PCA variance  : 0.9900806343052833\n",
      "PCA components: 143\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9800461771748452\n",
      "PCA components: 113\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9705962941618149\n",
      "PCA components: 96\n",
      "----->> Saving transformed dataset ...\n",
      "* qn.ds_MA\n",
      "PCA variance  : 0.9900549765192196\n",
      "PCA components: 179\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9801696984275255\n",
      "PCA components: 125\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9700125511613396\n",
      "PCA components: 98\n",
      "----->> Saving transformed dataset ...\n",
      "* qu.ds_MA\n",
      "PCA variance  : 0.990000538109783\n",
      "PCA components: 172\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9802589771076451\n",
      "PCA components: 120\n",
      "----->> Saving transformed dataset ...\n",
      "PCA variance  : 0.9700837065099048\n",
      "PCA components: 93\n",
      "----->> Saving transformed dataset ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# define the PCA variance you need\n",
    "PCA_vars = [0.99, 0.98, 0.97]\n",
    "# PCA_comps = 50 # no of PCA components\n",
    "\n",
    "# Reduce dimension of scaled datasets + the non-scaled one\n",
    "print('-> PCA reduction:')\n",
    "\n",
    "for prefix in scalerPrefix:   # for each prefix of file (all scaled and non-scaled files)\n",
    "    \n",
    "    # new file name; we will add tr and ts + csv\n",
    "    newFile = prefix+'.'+sOrigDataSet[:-4]\n",
    "    print('*', newFile)\n",
    "        \n",
    "    # read data train and test\n",
    "    newFile_tr = newFile +'_tr.csv'\n",
    "    df_tr = pd.read_csv(os.path.join(WorkingFolder, newFile_tr))\n",
    "    newFile_ts = newFile +'_ts.csv'\n",
    "    df_ts = pd.read_csv(os.path.join(WorkingFolder, newFile_ts))\n",
    "        \n",
    "    X_tr = df_tr.drop(outVar, axis = 1) # remove output variable from input features\n",
    "    y_tr = df_tr[outVar]                # get only the output variable\n",
    "    X_ts = df_ts.drop(outVar, axis = 1) # remove output variable from input features\n",
    "    y_ts = df_ts[outVar]                # get only the output variable\n",
    "\n",
    "    # get only the values for features and output (as arrays)\n",
    "    Xdata_tr = X_tr.values # get values of features\n",
    "    Ydata_tr = y_tr.values # get output values\n",
    "    Xdata_ts = X_ts.values # get values of features\n",
    "    Ydata_ts = y_ts.values # get output values\n",
    "\n",
    "    # for each PCA variance\n",
    "    for PCA_var in PCA_vars:    \n",
    "    \n",
    "        # apply reduction transform to training\n",
    "        #pca = PCA(n_components=noSelFeatures) # use PCA a number of new dimension\n",
    "        pca = PCA(PCA_var) # use PCA a number of new dimension\n",
    "        pca.fit(Xdata_tr)  # get transformation using training dataset\n",
    "        \n",
    "        #print(\"List of variance for each PCA component:\")\n",
    "        #print(pca.explained_variance_ratio_) # list of PCA component variance\n",
    "        print('PCA variance  :', sum(pca.explained_variance_ratio_))\n",
    "        print('PCA components:', pca.n_components_)\n",
    "\n",
    "        # Transform training data for training and test\n",
    "        X_tr_transf = pca.transform(Xdata_tr)\n",
    "        X_ts_transf = pca.transform(Xdata_ts)\n",
    "\n",
    "        # create a dataframe to save as traning and test\n",
    "        df_tr_transf = pd.DataFrame(data = X_tr_transf,\n",
    "                                    columns = [reduction+str(i) for i in range(1,pca.n_components_+1)])\n",
    "        df_ts_transf = pd.DataFrame(data = X_ts_transf,\n",
    "                                    columns = [reduction+str(i) for i in range(1,pca.n_components_+1)])\n",
    "\n",
    "        # add output feature values\n",
    "        df_tr_transf = pd.concat([df_tr_transf, y_tr], axis = 1)\n",
    "        df_ts_transf = pd.concat([df_ts_transf, y_ts], axis = 1)\n",
    "\n",
    "        # Save transformed training file\n",
    "        print('----->> Saving transformed dataset ...')\n",
    "        df_tr_transf.to_csv(os.path.join(WorkingFolder, reduction+str(PCA_var)+'.'+newFile_tr),\n",
    "                            index=False)\n",
    "        df_ts_transf.to_csv(os.path.join(WorkingFolder, reduction+str(PCA_var)+'.'+newFile_ts),\n",
    "                            index=False)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check if there is the same variance for some PCA! In the case with only 2 PCA dimension, there is no possible to decrease the variance from 0.99! (the files for all the variance are the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based Ranking\n",
    "\n",
    "We are applying only the following types of feature selection (FS):\n",
    "1. **RF feature selection** - We can fit a classfier to each feature and rank the predictive power. This method selects the most powerful features individually but ignores the predictive power when features are combined. Random Forest Classifier is used in this case because it is robust, nonlinear, and doesn't require scaling. \n",
    "2. **Univariate feature selection using chi-squared test**.\n",
    "3. **Univariate feature selection with mutual information**.\n",
    "\n",
    "Each dataset will be processed with all the FS methods and training and test files will be generated.\n",
    "\n",
    "*Note: We will not use FS for PCA! Only training set will be used for FS! Univariate methods need positive values as inputs!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> FEATURE SELECTION\n",
      "* Total features to keep with FS =  2\n",
      "* No of FS methods = 3\n",
      "* No of datasets   = 9\n"
     ]
    }
   ],
   "source": [
    "# Model Based Ranking\n",
    "# https://www.kaggle.com/dkim1992/feature-selection-ranking\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "\n",
    "# set no of features for drugs or for proteins (equal no for both types of descriptors)\n",
    "nFeats = int(noSelFeatures/2)\n",
    "\n",
    "# set description of each FS method\n",
    "FSnames = ['Random Forest feature selection',\n",
    "           'Univariate Feature Selection',\n",
    "           'Univariate feature selection with mutual information']\n",
    "\n",
    "# set sklearn methods with parameters\n",
    "FSmethods = [RandomForestClassifier(n_estimators = 50, max_depth = 4, n_jobs = -1),\n",
    "             SelectKBest(score_func=chi2, k=2),\n",
    "             SelectKBest(score_func = mutual_info_classif, k=2)]\n",
    "\n",
    "# prefix to add to the processed files for each FS method\n",
    "FSprefix = ['fs.rf.',\n",
    "            'fs.univchi.',\n",
    "            'fs.univmi.']\n",
    "\n",
    "print('-> FEATURE SELECTION')\n",
    "print('* Total features to keep with FS = ',nFeats)\n",
    "\n",
    "# get the list of files to be processed with FS RF \n",
    "# all training and test files except from PCA and FS!\n",
    "\n",
    "# listFiles_tr  = ['test_tr.csv','test2_tr.csv'] # manually set of training files\n",
    "listFiles_tr = [col for col in os.listdir(WorkingFolder) if ('tr' in col)\n",
    "                and ('pca' not in col) and ('fs' not in col)]\n",
    "\n",
    "# listFiles_ts  = ['test_ts.csv','test2_ts.csv'] # manually set of test files\n",
    "listFiles_ts = [col for col in os.listdir(WorkingFolder) if ('ts' in col)\n",
    "                and ('pca' not in col) and ('fs' not in col)]\n",
    "\n",
    "print('* No of FS methods =', len(FSmethods))\n",
    "print('* No of datasets   =', len(listFiles_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Processing:  m.ds_MA_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: m.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.m.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.rf.m.ds_MA_ts.csv ...\n",
      "* Processing:  m.ds_MA_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: m.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univchi.m.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univchi.m.ds_MA_ts.csv ...\n",
      "* Processing:  m.ds_MA_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: m.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.m.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.m.ds_MA_ts.csv ...\n",
      "* Processing:  o.ds_MA_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: o.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.o.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.rf.o.ds_MA_ts.csv ...\n",
      "* Processing:  o.ds_MA_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: o.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "!!! Error: fs.univchi.o.ds_MA_tr.csv\n",
      "* Processing:  o.ds_MA_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: o.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.o.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.o.ds_MA_ts.csv ...\n",
      "* Processing:  pyj.ds_MA_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: pyj.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.pyj.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.rf.pyj.ds_MA_ts.csv ...\n",
      "* Processing:  pyj.ds_MA_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: pyj.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "!!! Error: fs.univchi.pyj.ds_MA_tr.csv\n",
      "* Processing:  pyj.ds_MA_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: pyj.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.pyj.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.pyj.ds_MA_ts.csv ...\n",
      "* Processing:  qn.ds_MA_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: qn.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.qn.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.rf.qn.ds_MA_ts.csv ...\n",
      "* Processing:  qn.ds_MA_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: qn.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "!!! Error: fs.univchi.qn.ds_MA_tr.csv\n",
      "* Processing:  qn.ds_MA_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: qn.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.qn.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.qn.ds_MA_ts.csv ...\n",
      "* Processing:  qu.ds_MA_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: qu.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.qu.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.rf.qu.ds_MA_ts.csv ...\n",
      "* Processing:  qu.ds_MA_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: qu.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univchi.qu.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univchi.qu.ds_MA_ts.csv ...\n",
      "* Processing:  qu.ds_MA_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: qu.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.qu.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.qu.ds_MA_ts.csv ...\n",
      "* Processing:  r.ds_MA_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: r.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.r.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.rf.r.ds_MA_ts.csv ...\n",
      "* Processing:  r.ds_MA_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: r.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "!!! Error: fs.univchi.r.ds_MA_tr.csv\n",
      "* Processing:  r.ds_MA_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: r.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.r.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.r.ds_MA_ts.csv ...\n",
      "* Processing:  s.ds_MA_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: s.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.s.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.rf.s.ds_MA_ts.csv ...\n",
      "* Processing:  s.ds_MA_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: s.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "!!! Error: fs.univchi.s.ds_MA_tr.csv\n",
      "* Processing:  s.ds_MA_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: s.ds_MA_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.s.ds_MA_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.s.ds_MA_ts.csv ...\n",
      "* Processing:  test2_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: test2_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.test2_tr.csv ...\n",
      "---> Saving FS test set fs.rf.test2_ts.csv ...\n",
      "* Processing:  test2_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: test2_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "!!! Error: fs.univchi.test2_tr.csv\n",
      "* Processing:  test2_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: test2_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.test2_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.test2_ts.csv ...\n",
      "* Processing:  test_tr.csv | FS method:  Random Forest feature selection\n",
      "---> Reading data: test_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.rf.test_tr.csv ...\n",
      "---> Saving FS test set fs.rf.test_ts.csv ...\n",
      "* Processing:  test_tr.csv | FS method:  Univariate Feature Selection\n",
      "---> Reading data: test_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univchi.test_tr.csv ...\n",
      "---> Saving FS test set fs.univchi.test_ts.csv ...\n",
      "* Processing:  test_tr.csv | FS method:  Univariate feature selection with mutual information\n",
      "---> Reading data: test_tr.csv ...\n",
      "---> Calculate the scores for each feature ...\n",
      "---> Filter the scores for proteins ...\n",
      "---> Filter the scores for drugs ...\n",
      "---> Saving FS train set fs.univmi.test_tr.csv ...\n",
      "---> Saving FS test set fs.univmi.test_ts.csv ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# for each file apply each method of FS\n",
    "# no of resulting FS files = no input files * FS methods\n",
    "for f in range(len(listFiles_tr)):\n",
    "    newFile_tr = listFiles_tr[f]\n",
    "    newFile_ts = listFiles_ts[f]\n",
    "    \n",
    "    # for each file and method\n",
    "    for i in range(len(FSmethods)):\n",
    "        iFSnames   = FSnames[i]\n",
    "        iFSmethods = FSmethods[i]\n",
    "        iFSprefix  = FSprefix[i]\n",
    "        \n",
    "        print('* Processing: ', newFile_tr, '| FS method: ', iFSnames)\n",
    "    \n",
    "        # define the output files for FS RF as fsrf. + input file\n",
    "        newFile_FS_tr = iFSprefix + newFile_tr\n",
    "        newFile_FS_ts = iFSprefix + newFile_ts\n",
    "        \n",
    "        #### ADD checking if files exists, do not calculate again ???\n",
    "        #### ADD time for each transformation and print it ???\n",
    "        \n",
    "        # TRY - EXCEPT for possible errors\n",
    "        ### UNIV works only with positive X values!!!!!!!!!!!!!!!!\n",
    "        try:\n",
    "            # read training set\n",
    "            print('---> Reading data:', newFile_tr, '...')\n",
    "            df_tr = pd.read_csv(os.path.join(WorkingFolder, newFile_tr))\n",
    "            X_tr = df_tr.drop(outVar, axis = 1) # remove output variable from input features\n",
    "            y_tr = df_tr[outVar]                # get only the output variable\n",
    "\n",
    "            # for each method apply a specific function defined in iFSmethods\n",
    "            print('---> Calculate the scores for each feature ...')\n",
    "\n",
    "            # get the the scores using 10-fold CV\n",
    "            scores = []\n",
    "            FS = iFSmethods\n",
    "\n",
    "            # there are different evaluations for each FS method!\n",
    "            if iFSnames==FSnames[1] or iFSnames==FSnames[2]: # for univariate methods\n",
    "                FS.fit(X_tr, y_tr)\n",
    "\n",
    "            num_features = len(X_tr.columns)\n",
    "            for i in range(num_features):\n",
    "                # there are different evaluations for each FS method!\n",
    "                if iFSnames==FSnames[0]: # for RF\n",
    "                    col = X_tr.columns[i]\n",
    "                    score = np.mean(cross_val_score(FS, X_tr[col].values.reshape(-1,1), y_tr, cv=10))\n",
    "                if iFSnames==FSnames[1] or iFSnames==FSnames[2]: # for univariate methods\n",
    "                    score = FS.scores_[i]\n",
    "\n",
    "                # for all FS methods, append the scores\n",
    "                scores.append((int(score*100), col))\n",
    "\n",
    "            # create a dataframe with RF scores for each feature\n",
    "            df_scores = pd.DataFrame(sorted(scores, reverse = True), columns=['FS_score','FeatureName'])\n",
    "\n",
    "            # PROCESS PROTEIN DESCRIPTORS\n",
    "            # ----------------------------------\n",
    "            # get only the list for protein descriptors or define them manually!\n",
    "            protein_descriptors = [col for col in X_tr.columns if ('CHOC' in col) or ('BIGC' in col)\n",
    "                                   or ('CHAM' in col) or ('comp_' in col) ]\n",
    "\n",
    "            # create a dataframe with these names for proteins only\n",
    "            df_prot_descr = pd.DataFrame(protein_descriptors, columns=['FeatureName'])\n",
    "\n",
    "            print('---> Filter the scores for proteins ...')\n",
    "            # Get score only for proteins: merge feature names for protein with the score\n",
    "            df_protein_scores = pd.merge(df_prot_descr, df_scores, on=['FeatureName'])\n",
    "            df_protein_scores_sorted = df_protein_scores.sort_values('FS_score', ascending=False)\n",
    "\n",
    "            # get the best nFeats prot descriptors\n",
    "            BestProteinFeatures = list(df_protein_scores_sorted.FeatureName[:nFeats])\n",
    "\n",
    "            # PROCESS DRUG DESCRIPTORS\n",
    "            # ---------------------------------\n",
    "            # get the list of drug descriptors or create it manually!\n",
    "            drug_descriptors = [col for col in X_tr.columns if col not in protein_descriptors]\n",
    "\n",
    "            # create a dataframe with these names for proteins only\n",
    "            df_drug_descr = pd.DataFrame(drug_descriptors, columns=['FeatureName'])\n",
    "\n",
    "            print('---> Filter the scores for drugs ...')\n",
    "            # Get score for proteins: merge feature names for protein with the score\n",
    "            df_drug_scores = pd.merge(df_drug_descr, df_scores, on=['FeatureName'])\n",
    "            df_drug_scores_sorted = df_drug_scores.sort_values('FS_score', ascending=False)\n",
    "\n",
    "            # get the best nFeats drug descriptors\n",
    "            BestDrugFeatures =list(df_drug_scores_sorted.FeatureName[:nFeats])\n",
    "\n",
    "            # Get the list with drug and protein descriptors for the RF FS dataset\n",
    "            BestRFDescriptors = [y for x in [BestDrugFeatures, BestProteinFeatures] for y in x]\n",
    "\n",
    "            # Add output feature Lij\n",
    "            BestRFDescriptors.append('Lij')\n",
    "\n",
    "            # create feature selection dataframe\n",
    "            nds_fs = df_tr[BestRFDescriptors]\n",
    "\n",
    "            # Save feature selected training dataset\n",
    "            print('---> Saving FS train set', newFile_FS_tr,'...')\n",
    "            nds_fs.to_csv(os.path.join(WorkingFolder, newFile_FS_tr), index=False)\n",
    "\n",
    "            # get the same columns from the TS set and write the file!\n",
    "            # read test set\n",
    "            df_ts = pd.read_csv(os.path.join(WorkingFolder, newFile_ts))\n",
    "\n",
    "            # limit to only the selected features + output variable\n",
    "            df_ts = df_ts[BestRFDescriptors]\n",
    "\n",
    "            # Save feature selected test set\n",
    "            print('---> Saving FS test set', newFile_FS_ts,'...')\n",
    "            df_ts.to_csv(os.path.join(WorkingFolder, newFile_FS_ts), index=False)\n",
    "        except:\n",
    "            print('!!! Error:', newFile_FS_tr)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could different FS methods!\n",
    "https://www.kaggle.com/dkim1992/feature-selection-ranking\n",
    "\n",
    "In the next step, all the datasets will be used with baseline Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
